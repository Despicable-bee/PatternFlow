{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StyleGAN2-ADA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0nWQOOntpxe"
      },
      "source": [
        "# Training OASIS brain using StyleGAN2-ADA\n",
        "By Harry Nowakowski\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gXnARt0uQ_x"
      },
      "source": [
        "# Verify that our runtime is a GPU\n",
        "The great thing about google Colaboratory is you don't need to set up your own compute cluster (google is nice enough to provide one for you via web browser :D )\n",
        "\n",
        "In the menu, select Runtime -> Change Runtime Type\n",
        "\n",
        "Here you'll be able to verify if you're using a **GPU** (or even a **TPU** if you're feeling fancy).\n",
        "\n",
        "To verify that you're all set up, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBYNIEDIvhYi"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXJ_Ofrcvnne"
      },
      "source": [
        "You should get something like:\n",
        "\n",
        "`GPU 0: Tesla K80 (UUID: GPU-.....)`\n",
        "\n",
        "This means its working :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MZP-Kv1v2Wl"
      },
      "source": [
        "# Mount your Google Drive\n",
        "We'll be storing the training models and progress images on Google Drive (because this is a university report and it needs to get marked off).\n",
        "\n",
        "It also means that if you come from a place like Australia, if your Colab notebook gets disconnected, we won't lose the model (wow they really do think of everything).\n",
        "\n",
        "# EDIT:\n",
        "Google is also not made of money, so after 12 hours they'll probably take your GPU away (:c). To get around this, we can use our local GPU by running a [local runtime](https://research.google.com/colaboratory/local-runtimes.html).\n",
        "\n",
        "I'm using an Nvidia GeForce GTX 980, which only has 4GB of video memory (which is fine for games, but rubbish for ML).\n",
        "So it will take longer, however since CoLab pro isn't technically available in Australia, I have to make do.\n",
        "\n",
        "Doing this however will prevent you from using the `google.colab` library (so mounting your google drive will be a bit harder).\n",
        "\n",
        "However, now we have access to the RAM and Disk-space rich resource known as **Your own computer**.\n",
        "\n",
        "So you can just write to your local storage and everything will be fine :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDY2E24PwhzL"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhgCJgkZTJ0_"
      },
      "source": [
        "# Local dependencies\n",
        "If you're not using Google CoLab's online features anymore, you'll need to install pytorch and a bunch of other libs locally.\n",
        "\n",
        "Pytorch has a really nice UI for doing this as well, check out their website [here](https://pytorch.org/get-started/locally/). It allows you to pick your installation options from the UI! (nifty!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ZTdvuGTuGH"
      },
      "source": [
        "!pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio===0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpyTOuGix8dx"
      },
      "source": [
        "# Install StyleGAN2-ada pytorch prerequisites\n",
        "The black magic that makes the wizz bizz happen ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ9fYZmiyJYo"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdrlUrZryN3G"
      },
      "source": [
        "Check that pytorch has recognised our connected graphics card.\n",
        "\n",
        "To do this, run the following code, and we should get \"1\" in the console output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LxgflTKyeZR",
        "outputId": "0d5cbf15-dc24-453d-acde-567d39aec1f1"
      },
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzgWhhZAyjC5"
      },
      "source": [
        "import torchvision"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOiFmLuyynOs"
      },
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPcZArCEyxLp"
      },
      "source": [
        "You're probably wondering what we just installed, let me explain\n",
        "- **torchvision**: A package that contains a bunch of popular datasets, model architectures, and image transformations for computer vision (so things like the `EMNIST` dataset and so on)\n",
        "- **click**: \"Command Line Interface Creation Kit\", or \"CLICK\", is a package that enables the creation of command line interfaces (more beautifully and more easily)\n",
        "- **requests**: The requests library, it allows us to send and recieve requests via HTTP.\n",
        "- **tqdm**: One of my favourites, tqdm is a smart progress meter package. You can include these in your loops to show how things are progressing in your application (which you'll definitely need for StyleGAN).\n",
        "- **pyspng**: Fast (and efficient) png decoder. It quickly and efficiently loads PNG files into numpy arrays.\n",
        "- **ninja**: A small build system (with a focus on speed). High-level languages are slow as hell, so ninja aims to be \"The assembler for python\"\n",
        "- **imageio-ffmpeg**: FFMPEG wrapper for python (necessary when we want to make videos from a bunch of images). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ1OtQ6k1STV"
      },
      "source": [
        "# Getting the StyleGAN code\n",
        "\n",
        "Even though this is technically the hardest project on the projects sheet of paper, the worst of the demon magic is mostly done for us with the styleGAN2 package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmEWwVcB1w4P"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiyfmQlD157t"
      },
      "source": [
        "mkdir /content/stylegan2-ada-pytorch/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSpMi4jM2GR7",
        "outputId": "e7150850-8305-4f17-cc64-4b7d7abe2802"
      },
      "source": [
        "cd /content/stylegan2-ada-pytorch/datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada-pytorch/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcCFtisabbqs"
      },
      "source": [
        "Check we're in the right directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmsCHTltbDup"
      },
      "source": [
        "!dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzkGOnY32M_j"
      },
      "source": [
        "# Getting the OASIS brain dataset of images\n",
        "Lets get the dataset from blackboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btSGlYso7Zgp"
      },
      "source": [
        "!wget -c https://cloudstor.aarnet.edu.au/plus/s/tByzSZzvvVh0hZA/download -O oasis-preproc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-J2YjMbjiW"
      },
      "source": [
        "Or, if you're running this locally on windows, you can use the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwKtJSEzbna0"
      },
      "source": [
        "!curl https://cloudstor.aarnet.edu.au/plus/s/tByzSZzvvVh0hZA/download -o oasis-preproc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_3CEFkg8l_W"
      },
      "source": [
        "Now unzip the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dge_dXi18noi"
      },
      "source": [
        "!unzip oasis-preproc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GT4SqcAegxV"
      },
      "source": [
        "Or if you're running this on windows, use the following (For windows 10 build 17063 or later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh4422W9ekq_"
      },
      "source": [
        "!tar -xf oasis-preproc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLqaXF985dZ"
      },
      "source": [
        "# Prepare OASIS dataset for use by styleGAN\n",
        "While UQ was nice enough to preprocess the image data for us, it still isn't in the standard form that styleGAN expects (000001.png, 000002.png, etc)\n",
        "\n",
        "Luckily, the (genius) folks at Nvidia have thought of this already, and included a nice `dataset_tool.py` file to do this formatting and conversion for us :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIi0qelw9tLL",
        "outputId": "66032214-e743-40fd-a424-2ffba6c09c3e"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJaXVIfq9ykR",
        "outputId": "34cb5f5a-a90d-45ad-ec95-fc6a7db240ae"
      },
      "source": [
        "!python dataset_tool.py --source=./datasets/keras_png_slices_data/keras_png_slices_train --dest=./datasets/oasis-stylegan-dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 9664/9664 [00:19<00:00, 485.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5m10Cdp-Mu6"
      },
      "source": [
        "You may be wondering:\n",
        "\n",
        "\"Harry what are you doing!? you've saved all those images into a `.zip` folder?!\"\n",
        "\n",
        "Yes.\n",
        "\n",
        "If you check the **Compatibility** section on styleGAN2-ada-pytorch's [Github page](https://github.com/NVlabs/stylegan2-ada-pytorch), you'll come across the following:\n",
        "\n",
        "\"*New ZIP/PNG based dataset format for maximal interoperability with existing 3rd party tools*\"\n",
        "\n",
        "As well as\n",
        "\n",
        "\"*TFRecords datasets are no longer supported — they need to be converted to the new format.*\"\n",
        "\n",
        "What this means is we no longer have to store images in the proprietary `.tfr` format in order to load images into styleGAN.\n",
        "\n",
        "This is great because it means I can be MORE lazy (thank you giga chads at Nvidia).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9GA4pzbBtZ3"
      },
      "source": [
        "# Create (progress) Folders on Google Drive\n",
        "In the event that we accidentally close our browser, or the Colab runtime disconnects (because we were idle for too long), we will lose all of our training models and progress images :v .\n",
        "\n",
        "To prevent this, the Giga chads at Nvidia have done it again.\n",
        "\n",
        "Periodically, styleGAN2 will **pickle** our model.\n",
        "\n",
        "We can save this pickle to Google Drive and resume training at a later date if we want.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI0j9QXjCdgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a44acb-ffae-4129-f367-3958ad239d9f"
      },
      "source": [
        "mkdir /content/drive/MyDrive/COMP3710_report"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/COMP3710_report’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XsGrChGCp7-"
      },
      "source": [
        "mkdir /content/drive/MyDrive/COMP3710_report/OASIS_training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtXYiG6PC29K"
      },
      "source": [
        "# Time to T R A I N (the ride never ends...)\n",
        "Here we summon the demons from Nvidia's basement, make a deal with them, and then have them train our model.\n",
        "\n",
        "These demons don't speak english though, so we have to communicate with them using the following incantation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcalsYBODRcg",
        "outputId": "b0e5ab82-8928-49e6-d169-fc2e83660bb0"
      },
      "source": [
        "!python train.py --outdir=/content/drive/MyDrive/COMP3710_report/OASIS_training_data --data=./datasets/oasis-stylegan-dataset.zip --gpus=1 --augpipe=bg --gamma=10 --cfg=paper256 --mirror=1 --snap=10 --metrics=none"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"./datasets/oasis-stylegan-dataset.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 9664,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 8\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 10.0\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 64,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 20,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"run_dir\": \"/content/drive/MyDrive/COMP3710_report/OASIS_training_data/00005-oasis-stylegan-dataset-mirror-paper256-gamma10-bg\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/COMP3710_report/OASIS_training_data/00005-oasis-stylegan-dataset-mirror-paper256-gamma10-bg\n",
            "Training data:      ./datasets/oasis-stylegan-dataset.zip\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   9664\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "Num images:  19328\n",
            "Image shape: [1, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/drive/MyDrive/COMP3710_report/OASIS_training_data/00004-oasis-stylegan-dataset-mirror-paper256-gamma10-bg/network-snapshot-000096.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape        Datatype\n",
            "---                   ---         ---      ---                 ---     \n",
            "mapping.fc0           262656      -        [8, 512]            float32 \n",
            "mapping.fc1           262656      -        [8, 512]            float32 \n",
            "mapping.fc2           262656      -        [8, 512]            float32 \n",
            "mapping.fc3           262656      -        [8, 512]            float32 \n",
            "mapping.fc4           262656      -        [8, 512]            float32 \n",
            "mapping.fc5           262656      -        [8, 512]            float32 \n",
            "mapping.fc6           262656      -        [8, 512]            float32 \n",
            "mapping.fc7           262656      -        [8, 512]            float32 \n",
            "mapping               -           512      [8, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    263169      -        [8, 1, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    263169      -        [8, 1, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   263169      -        [8, 1, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   263169      -        [8, 1, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [8, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   131585      -        [8, 1, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [8, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [8, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  65793       -        [8, 1, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [8, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [8, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  32897       -        [8, 1, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [8, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [8, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                 ---     \n",
            "Total                 24762452    175568   -                   -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
            "---            ---         ---      ---                 ---     \n",
            "b256.fromrgb   128         16       [8, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [8, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [8, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [8, 128, 128, 128]  float16 \n",
            "b256           -           16       [8, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [8, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [8, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [8, 256, 64, 64]    float16 \n",
            "b128           -           16       [8, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [8, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [8, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [8, 512, 32, 32]    float16 \n",
            "b64            -           16       [8, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [8, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [8, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [8, 512, 16, 16]    float16 \n",
            "b32            -           16       [8, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [8, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n",
            "b16            -           16       [8, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [8, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n",
            "b8             -           16       [8, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [8, 512]            float32 \n",
            "b4.out         513         -        [8, 1]              float32 \n",
            "---            ---         ---      ---                 ---     \n",
            "Total          24000961    416      -                   -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      time 34s          sec/tick 9.5     sec/kimg 148.75  maintenance 24.6   cpumem 4.85   gpumem 8.79   augment 0.000\n",
            "tick 1     kimg 4.1      time 5m 37s       sec/tick 294.1   sec/kimg 72.95   maintenance 8.8    cpumem 5.09   gpumem 2.86   augment 0.000\n",
            "tick 2     kimg 8.1      time 10m 34s      sec/tick 297.3   sec/kimg 73.75   maintenance 0.0    cpumem 5.09   gpumem 2.86   augment 0.000\n",
            "tick 3     kimg 12.2     time 15m 32s      sec/tick 297.4   sec/kimg 73.76   maintenance 0.1    cpumem 5.09   gpumem 2.86   augment 0.000\n",
            "tick 4     kimg 16.2     time 20m 29s      sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.09   gpumem 2.86   augment 0.002\n",
            "tick 5     kimg 20.2     time 25m 25s      sec/tick 295.8   sec/kimg 73.36   maintenance 0.1    cpumem 5.09   gpumem 2.86   augment 0.004\n",
            "tick 6     kimg 24.3     time 30m 22s      sec/tick 297.3   sec/kimg 73.74   maintenance 0.0    cpumem 5.09   gpumem 2.87   augment 0.001\n",
            "tick 7     kimg 28.3     time 35m 20s      sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.09   gpumem 2.86   augment 0.001\n",
            "tick 8     kimg 32.3     time 40m 17s      sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.09   gpumem 2.86   augment 0.001\n",
            "tick 9     kimg 36.4     time 45m 13s      sec/tick 295.8   sec/kimg 73.35   maintenance 0.1    cpumem 5.09   gpumem 2.86   augment 0.000\n",
            "tick 10    kimg 40.4     time 50m 10s      sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.09   gpumem 2.86   augment 0.000\n",
            "tick 11    kimg 44.4     time 55m 17s      sec/tick 297.4   sec/kimg 73.75   maintenance 9.0    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 12    kimg 48.4     time 1h 00m 14s   sec/tick 297.2   sec/kimg 73.72   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 13    kimg 52.5     time 1h 05m 10s   sec/tick 295.7   sec/kimg 73.34   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 14    kimg 56.5     time 1h 10m 07s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 15    kimg 60.5     time 1h 15m 04s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 16    kimg 64.6     time 1h 20m 02s   sec/tick 297.2   sec/kimg 73.72   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 17    kimg 68.6     time 1h 24m 56s   sec/tick 293.9   sec/kimg 72.90   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 18    kimg 72.6     time 1h 29m 53s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.0    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 19    kimg 76.7     time 1h 34m 50s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 20    kimg 80.7     time 1h 39m 48s   sec/tick 297.2   sec/kimg 73.72   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 21    kimg 84.7     time 1h 44m 52s   sec/tick 295.8   sec/kimg 73.36   maintenance 8.6    cpumem 5.55   gpumem 2.86   augment 0.001\n",
            "tick 22    kimg 88.8     time 1h 49m 49s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.55   gpumem 2.86   augment 0.000\n",
            "tick 23    kimg 92.8     time 1h 54m 47s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.55   gpumem 2.86   augment 0.001\n",
            "tick 24    kimg 96.8     time 1h 59m 44s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.55   gpumem 2.86   augment 0.000\n",
            "tick 25    kimg 100.9    time 2h 04m 40s   sec/tick 295.7   sec/kimg 73.35   maintenance 0.1    cpumem 5.55   gpumem 2.86   augment 0.000\n",
            "tick 26    kimg 104.9    time 2h 09m 37s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.0    cpumem 5.55   gpumem 2.86   augment 0.000\n",
            "tick 27    kimg 108.9    time 2h 14m 35s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.55   gpumem 2.86   augment 0.000\n",
            "tick 28    kimg 113.0    time 2h 19m 32s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.55   gpumem 2.86   augment 0.000\n",
            "tick 29    kimg 117.0    time 2h 24m 28s   sec/tick 295.7   sec/kimg 73.35   maintenance 0.1    cpumem 5.55   gpumem 2.86   augment 0.001\n",
            "tick 30    kimg 121.0    time 2h 29m 25s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.55   gpumem 2.86   augment 0.000\n",
            "tick 31    kimg 125.1    time 2h 34m 31s   sec/tick 297.3   sec/kimg 73.75   maintenance 8.4    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 32    kimg 129.1    time 2h 39m 28s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 33    kimg 133.1    time 2h 44m 23s   sec/tick 294.0   sec/kimg 72.92   maintenance 0.1    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 34    kimg 137.2    time 2h 49m 20s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.0    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 35    kimg 141.2    time 2h 54m 17s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 36    kimg 145.2    time 2h 59m 15s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 37    kimg 149.2    time 3h 04m 11s   sec/tick 295.8   sec/kimg 73.35   maintenance 0.1    cpumem 5.45   gpumem 2.86   augment 0.001\n",
            "tick 38    kimg 153.3    time 3h 09m 08s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 39    kimg 157.3    time 3h 14m 05s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 40    kimg 161.3    time 3h 19m 03s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.45   gpumem 2.86   augment 0.000\n",
            "tick 41    kimg 165.4    time 3h 24m 07s   sec/tick 295.9   sec/kimg 73.38   maintenance 8.3    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 42    kimg 169.4    time 3h 29m 04s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.0    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 43    kimg 173.4    time 3h 34m 01s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 44    kimg 177.5    time 3h 38m 59s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.1    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 45    kimg 181.5    time 3h 43m 55s   sec/tick 295.7   sec/kimg 73.35   maintenance 0.1    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 46    kimg 185.5    time 3h 48m 52s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 47    kimg 189.6    time 3h 53m 49s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 48    kimg 193.6    time 3h 58m 47s   sec/tick 297.2   sec/kimg 73.72   maintenance 0.1    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 49    kimg 197.6    time 4h 03m 41s   sec/tick 293.9   sec/kimg 72.90   maintenance 0.1    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 50    kimg 201.7    time 4h 08m 38s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.34   gpumem 2.86   augment 0.000\n",
            "tick 51    kimg 205.7    time 4h 13m 44s   sec/tick 297.3   sec/kimg 73.74   maintenance 8.6    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 52    kimg 209.7    time 4h 18m 41s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.1    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 53    kimg 213.8    time 4h 23m 37s   sec/tick 295.7   sec/kimg 73.34   maintenance 0.1    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 54    kimg 217.8    time 4h 28m 34s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.33   gpumem 2.86   augment 0.001\n",
            "tick 55    kimg 221.8    time 4h 33m 32s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 56    kimg 225.9    time 4h 38m 29s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 57    kimg 229.9    time 4h 43m 25s   sec/tick 295.8   sec/kimg 73.35   maintenance 0.1    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 58    kimg 233.9    time 4h 48m 22s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.0    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 59    kimg 238.0    time 4h 53m 20s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 60    kimg 242.0    time 4h 58m 17s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.33   gpumem 2.86   augment 0.000\n",
            "tick 61    kimg 246.0    time 5h 03m 21s   sec/tick 295.8   sec/kimg 73.35   maintenance 8.2    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 62    kimg 250.0    time 5h 08m 18s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 63    kimg 254.1    time 5h 13m 16s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 64    kimg 258.1    time 5h 18m 13s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 65    kimg 262.1    time 5h 23m 07s   sec/tick 294.0   sec/kimg 72.90   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 66    kimg 266.2    time 5h 28m 04s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 67    kimg 270.2    time 5h 33m 02s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 68    kimg 274.2    time 5h 37m 59s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 69    kimg 278.3    time 5h 42m 55s   sec/tick 295.7   sec/kimg 73.34   maintenance 0.1    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 70    kimg 282.3    time 5h 47m 52s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.0    cpumem 5.44   gpumem 2.86   augment 0.000\n",
            "tick 71    kimg 286.3    time 5h 52m 58s   sec/tick 297.3   sec/kimg 73.74   maintenance 8.5    cpumem 5.43   gpumem 2.86   augment 0.000\n",
            "tick 72    kimg 290.4    time 5h 57m 55s   sec/tick 297.2   sec/kimg 73.72   maintenance 0.1    cpumem 5.43   gpumem 2.86   augment 0.000\n",
            "tick 73    kimg 294.4    time 6h 02m 51s   sec/tick 295.7   sec/kimg 73.35   maintenance 0.1    cpumem 5.43   gpumem 2.86   augment 0.000\n",
            "tick 74    kimg 298.4    time 6h 07m 48s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.43   gpumem 2.86   augment 0.000\n",
            "tick 75    kimg 302.5    time 6h 12m 46s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.43   gpumem 2.86   augment 0.000\n",
            "tick 76    kimg 306.5    time 6h 17m 43s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.43   gpumem 2.86   augment 0.000\n",
            "tick 77    kimg 310.5    time 6h 22m 39s   sec/tick 295.7   sec/kimg 73.33   maintenance 0.1    cpumem 5.43   gpumem 2.86   augment 0.000\n",
            "tick 78    kimg 314.6    time 6h 27m 36s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.40   gpumem 2.86   augment 0.000\n",
            "tick 79    kimg 318.6    time 6h 32m 33s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.40   gpumem 2.86   augment 0.000\n",
            "tick 80    kimg 322.6    time 6h 37m 31s   sec/tick 297.2   sec/kimg 73.72   maintenance 0.1    cpumem 5.40   gpumem 2.86   augment 0.000\n",
            "tick 81    kimg 326.7    time 6h 42m 33s   sec/tick 294.0   sec/kimg 72.92   maintenance 8.1    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 82    kimg 330.7    time 6h 47m 30s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 83    kimg 334.7    time 6h 52m 28s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 84    kimg 338.8    time 6h 57m 25s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 85    kimg 342.8    time 7h 02m 21s   sec/tick 295.7   sec/kimg 73.34   maintenance 0.1    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 86    kimg 346.8    time 7h 07m 18s   sec/tick 297.3   sec/kimg 73.72   maintenance 0.0    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 87    kimg 350.8    time 7h 12m 16s   sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 88    kimg 354.9    time 7h 17m 13s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.1    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 89    kimg 358.9    time 7h 22m 09s   sec/tick 295.7   sec/kimg 73.35   maintenance 0.1    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "tick 90    kimg 362.9    time 7h 27m 06s   sec/tick 297.3   sec/kimg 73.73   maintenance 0.0    cpumem 5.42   gpumem 2.86   augment 0.000\n",
            "\n",
            "Aborted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B14cC2-9hrLb"
      },
      "source": [
        "(And use this in the event the demons speak windows)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muBXXXWLhu-X"
      },
      "source": [
        "!python train.py --outdir=OASIS_training_data --data=./datasets/oasis-stylegan-dataset.zip --gpus=1 --augpipe=bg --gamma=10 --cfg=paper256 --mirror=1 --snap=10 --metrics=none"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tswUmPWND5BR"
      },
      "source": [
        "Some of these parameters might look a little confusing, so I'll explain:\n",
        "- `--gpus=1`: The number of GPUs we're using (default is 1)\n",
        "- `--augpipe=bg`: Augmentation pipeline. This parameter has a lot of subtleties, as it dictates what the discriminator is allowed to augment (hence the ADA part of the styleGAN2 package). the options we're using are `bg`, which mean we're enabling pixel blitting, and geometric augmentations, but disabling colour, filter, noise, and cutout.\n",
        "- `--gamma=10`: Overrides R1 gamma\n",
        "- `--cfg=paper256`: Sets the configuration of the output. here, the `paper256` instructs the generator to produce images at 256x256 pixels.\n",
        "- `--mirror=1`: Amplifies the dataset with x-flips (in this case 1). Often beneficial, even with ADA as it introduces more variation.\n",
        "- `--snap=10`: Snapshot interval, controls how many ticks between saving a snapshot to the Google Drive.\n",
        "- `--metrics=none`: For each pickle, Frechet Inception Distance (FID) is evaluated and the score is logged in `metric-fid...json`. Since we don't really care how 'good' the model is, we're not going to store that data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wqQvecHHJHA"
      },
      "source": [
        "# And now, we wait... for hours :v\n",
        "Training can take DAYS, WEEKS, or even MONTHS (imagine trying to train a discriminator for self driving cars lol)\n",
        "\n",
        "Results will be stored in the `drive/MyDrive/COMP3710_report/OASIS_training_data` folder (images and pickles).\n",
        "\n",
        "Each time you run the above code, it will store the results in a new directory.\n",
        "(e.g. first time you run it will store the results in `00000-whateverYouNamedThisThing...`. And then the next time you run, it will store it in `00001-whateverYouNamedThisThing...`, and so on.\n",
        "\n",
        "Inside these directories, you'll see a bunch of files.\n",
        "- `real.png`: Shows a sample of the training dataset (in a nice mosaich layout).\n",
        "- `fakes00000.png`: Shows a sample of the generated images produced by the generator (also in a nice mosaich layout).\n",
        "- `network-snapshot-X.pkl` is the pickled model which we use to generate all those 'fake' images.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGtEZPvFnGGT"
      },
      "source": [
        "# This is taking too long!\n",
        "You'll probably reach a point where you've been waiting for a million years and the job still isn't done.\n",
        "\n",
        "That's OK!, we can make the computer go faster by leveraging UQs **GOLIATH HPC cluster**\n",
        "\n",
        "In order to communicate with the cluster, we need to write what's called a `SLURM` script.\n",
        "\n",
        "A `SLURM` script is just a bash file with a bunch of instructions that the cluster reads in order to setup the environment.\n",
        "\n",
        "If you're lazy like me, you can use [this](https://www.hpc.iastate.edu/guides/classroom-hpc-cluster/slurm-job-script-generator) nifty website in order to generate a script for you :)\n",
        "\n",
        "Now, you might be wondering.\n",
        "\n",
        "Where are all my files going to go?\n",
        "\n",
        "The answer to that is Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEv8YXDZEkQE"
      },
      "source": [
        "# A bit about SLURM\n",
        "SLURM\n",
        "\n",
        "Slurm consists of a daemon (called `slurmd`) running on each compute node, and a central daemon (called `slurmctld`) running on a management node (with an option fail-over twin, similar to webservers).\n",
        "\n",
        "The `slurmd` daemons provide fault-tolerant hierarchical communications.\n",
        "\n",
        "Information can be queried using several user commands:\n",
        "- `sacct`: Displays accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database ([more info](https://slurm.schedmd.com/sacct.html))\n",
        "- `salloc`: Obtain a Slurb job allocation (a set of nodes), execute a command, and then release the allocation when the command is finished [more info](https://slurm.schedmd.com/salloc.html))\n",
        "- `sattach`: Attaches to a running slurm job step. By attaching, it makes available IO streams of all the tasks of a running Slurm job step. It is also suitable for use with a parallel debugger like `TotalView` ([more info](https://slurm.schedmd.com/sattach.html))\n",
        "- `sbatch`: Submits a batch script to Slurm. The batch script may be given to sbatch through a file name on the command line, or if nothing is provided, sbatch will read in a script from standard input (useful for piping). The batch script is the file with all those `#SBATCH` directives at the top [more info](https://slurm.schedmd.com/sbatch.html). The script will typically contain one or more srun commands to launch parallel tasks.\n",
        "- `sbcast`: Used to transmit a file to all nodes allocated to the current active Slurm job. This command should only be executed from within a Slurm batch job or within the shell spawned after a Slurm Job's resource allocation [more info](https://slurm.schedmd.com/sbcast.html). It can also be used to transfer a file from local disk to local disk on the nodes allocated to a job. This can be used to effectively use diskless compute nodes or provide improved performance relative to a shared file system.\n",
        "- `scancel`: Used to signal (oooo CSSE2310 signals) or cancel jobs, job arrays or job steps. An Arbitrary number of jobs or job steps may be signaled using job specific filters or a space separated list of specific job and/or job step IDs [more info](https://slurm.schedmd.com/scancel.html).\n",
        "- `scontrol`: used to view or modify Slurm configurations including: job, job step, node, partition, reservation, and overall system configuration (NOTE, most commands can only be executed by an administrator) [more info](https://slurm.schedmd.com/scontrol.html)\n",
        "- `sinfo`: used to view partition and node information for a system running Slurm (so state information) [more info](https://slurm.schedmd.com/sinfo.html)\n",
        "- `smap`: Graphically view information about slurm jobs, partitions, and set configuration parameters (nice).\n",
        "- `squeue`: Used to view job and job step information for jobs managed by Slurm. This is different to sinfo as it has a wider variety of filtering, sorting, and formatting options. By default, it reports the running jobs in priority order and then the pending jobs in priority order [more info](https://slurm.schedmd.com/squeue.html).\n",
        "- `srun`: Run a parallel job on cluster managed by Slurm. If necessary, srun will first create a resource allocation in which to run the parallel job. It can be used to submit a job for execution, or initiate job steps in real time. It has a bunch of options for specifying resource requirements. (Also note that a job can contain **multiple jobs** executing **sequentially** or in **parallel** on independent or shared resources within the job's node allocation) [more info](https://slurm.schedmd.com/srun.html).\n",
        "- `strigger`: Used to set, get or view Slurm trigger information. Triggers include events such as node failing (going down), a job reaching its time limit or a job terminating. These events can cause actions such as the execution of an arbitrary script [more info](https://slurm.schedmd.com/strigger.html).\n",
        "- `sview`: Used to view Slurm configuration, job, step, node and paritions state information (all in a nice GUI). Authorized users can also modify select information (cool for debugging). Also note that this requires **GTK** to be installed, which may or may not be available on the system [more info](https://slurm.schedmd.com/sview.html).\n",
        "\n",
        "The entities managed by these Slurm daemons include\n",
        "- **nodes**: The compute resource in Slurm\n",
        "- **partitions**: group nodes into logical (possibly overlapping) sets, jobs or allocations of resources assigned to a user for a specified amount of time\n",
        "- **job steps**: sets of (possibly parallel) tasts within a job.\n",
        "\n",
        "Partitions can be considered **job queues**.\n",
        "\n",
        "Each partition is constrained by a number of factors (oh boy more lagrange stuff)\n",
        "- job size limit\n",
        "- job time limit\n",
        "- users permitted to use it\n",
        "- etc.\n",
        "\n",
        "**Priority-ordered jobs** are **allocated nodes** within a partition until the resources (nodes, processors, memory, etc) within that partition are exhausted.\n",
        "\n",
        "Once a job is assigned a set of nodes, the user is able to initiate parallel work in the form of job steps in any configuration within the allocation.\n",
        "\n",
        "For example:\n",
        "- A single job step may be started that utilizes all nodes allocated to the job.\n",
        "- Several job steps may independently use a portion of the allocation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eraMHEhNvkVx"
      },
      "source": [
        "# CREATING A JOB\n",
        "A job is formed of two sections: **resource request** and **job steps**.\n",
        "\n",
        "**resource requests** involves specifying:\n",
        "- The required number of CPUs/GPUs\n",
        "- Expected job duration\n",
        "- Amounts of RAM\n",
        "- Disk space\n",
        "- etc.\n",
        "\n",
        "**Job steps** involve describing what needs to be done (i.e. computing steps, which software to run, parameter space, etc).\n",
        "\n",
        "Typically a job is created via a submission script (e.g. a `.sh` script).\n",
        "\n",
        "The very first lione of the submission file has to be the bashbang (e.g. `#!/bin/bash`). Then the next lines must be the `SBATCH` directives. Finally, you can input any other line.\n",
        "\n",
        "For example, **comments** (a line starting with `#`) **prefixed with SBATCH** (i.e. `#SBATCH`) at the beginning of a bash script are understood by SLURM as **paramters describing resource requests and other submission options**\n",
        "\n",
        "The script itself is a job step. Other job steps are created with the srun command.\n",
        "\n",
        "Example (we'll call this script `submit.sh`):\n",
        "```\n",
        "#!/bin/bash\n",
        "#\n",
        "#SBATCH --job-name=test\n",
        "#SBATCH --output=res.txt\n",
        "#\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --time=60:00\n",
        "#SBATCH --mem-per-cpu=200\n",
        "\n",
        "srun hostname\n",
        "srun sleep 60\n",
        "```\n",
        "Now was submit this job to the queue\n",
        "When we hit enter, we'll get a message saying the job has been submitted, along with a job id.\n",
        "```\n",
        "> sbatch submit.sh\n",
        "sbatch:  Submitted batch job 99999999\n",
        "```\n",
        "\n",
        "Once a job has been submitted to a queue with `sbatch`, execution will follow these steps/states:\n",
        "- **PENDING**: The job then enters the queue in the PENDING state\n",
        "- **RUNNING**: Once resources become available, and the job has highest priority, an allocation is created for it, and it goes to the RUNNING state.\n",
        "- If the job completes correctly, it goes to the **COMPLETED** state, otherwise, it is set to the **FAILED** state.\n",
        "\n",
        "# PARALLEL JOBS\n",
        "Parallel jobs (e.g. tasks ran simultaneously) can be created via a different method (this isn't relevant for this section, but is cool to know).\n",
        "\n",
        "Examples of multi-process jobs include:\n",
        "- **A Multi-process program** (Single process, multiple data (SPMD) paradigm, e.g. with MPI)\n",
        "- **A Multi-threaded program** (Shared memory paradigm, e.g. with OpenMP or pthreads)\n",
        "- **Several instances of a single-threaded program**: (Embarassingly parallel paradigm or a job array)\n",
        "- **One master pgoram controlling several slave programs**: (master/slave paradigm)\n",
        "\n",
        "In the context of SLURM,\n",
        "- A task represents a process\n",
        "- A multi-process program is made of several tasks\n",
        "- By contrast, a multi-threaded program is composed of only one task, which uses several CPUs.\n",
        "\n",
        "Tasks are requested/created with the `--ntasks` option, while CPUs, for the multithreaded programs, are requested with the `--cpus-per-task` option.\n",
        "- Tasks cannot be split across several compute nodes, so requesting several CPUs with the `--cpus-per-task` option will ensure all CPUs are allocated on the same compute node.\n",
        "- By contrast, requesting the same amount of CPUs with the `--ntask` option may lead to several CPUs being allocated on several distinct compute nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr6PG5p62FBn"
      },
      "source": [
        "# SCRIPT EXAMPLES\n",
        "As none of us 100% understand whats going on unless we can see patterns in the examples, here are some cool examples of SLURM scripts\n",
        "\n",
        "# EXAMPLE 1. MPI\n",
        "Let's begin with a simple MPI example: Hello world.\n",
        "\n",
        "Wikipedia has a nice [example](https://en.wikipedia.org/wiki/Message_Passing_Interface#Example_program) of a working MPI program, so let's just copy that.\n",
        "Save this code as `wiki_mpi_example.c`\n",
        "\n",
        "Next we need to make an `sbatch script`\n",
        "We can either compile the program in advance (possibly better) or compile it before running the code in the sbatch script\n",
        "\n",
        "Because I usually forget how to write `Makefile`'s, we're going to do this in the sbatch script, however in practise, you'd write, debug and compile your program on your own PC first.\n",
        "\n",
        "We can send this script to the queue by executing the command\n",
        "```\n",
        "sbatch example_mpi.sbatch\n",
        "```\n",
        "Our `sbatch` script will look like the following (call the script `example_mpi.sbatch`):\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "#SBATCH --job-name=test_mpi\n",
        "#SBATCH --output=res_mpi.txt\n",
        "\n",
        "# Request 4 CPUS\n",
        "#SBATCH -ntasts=4\n",
        "\n",
        "# Request 10 minutes of compute time\n",
        "#SBATCH --time=10:00\n",
        "\n",
        "# Request 100 MiB of memory per CPU\n",
        "#SBATCH --mem-per-cpu=100\n",
        "\n",
        "# Modules that are installed on the node (similar to the modules you get via sudo apt-get install ...)\n",
        "module load gcc/6.4.0\n",
        "module load openmpi/3.0.0\n",
        "\n",
        "# Compile the C program\n",
        "mpicc wiki_mpi_example.c -o hello.mpi\n",
        "\n",
        "# Launch the mpi program\n",
        "srun hello.mpi\n",
        "```\n",
        "\n",
        "Then we can submit this to the queue using the above command.\n",
        "\n",
        "# EXAMPLE 2. GPU Job\n",
        "Similar idea as example 1, but the script will look like this\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=gpu_test\n",
        "#SBATCH --output=res_gpu.out\n",
        "#SBATCH --error=res_gpu.err\n",
        "\n",
        "#SBATCH --mail-type=ALL\n",
        "#SBATCH --mail-user=email@address.com\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --ntasks=8\n",
        "#SBATCH --cpus-per-task=1\n",
        "#SBATCH --ntasts-per-node=8\n",
        "\n",
        "#SBATCH --distribution=cyclic:cyclic\n",
        "#SBATCH --mem-per-cpu=7000mb\n",
        "#SBATCH --partition=gpu\n",
        "#SBATCH --gpus:tesla:4\n",
        "#SBATCH --time=00:30:00\n",
        "\n",
        "module purge\n",
        "module load cuda/10.0.130 intel/2018 openmpi/4.0.0 vasp/5.4.4\n",
        "\n",
        "srun --mpi=pmix_v3 vasp_gpu\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhA1DkK5AHi9"
      },
      "source": [
        "# Enough Talk, show us some brains.\n",
        "For that, you're going to have to run the\n",
        "```\n",
        "test_script.py\n",
        "```\n",
        "file.\n",
        "\n",
        "This file uses the generator from our latest pre-trained network snapshot.\n",
        "\n",
        "This allows us to generate nice pictures of brains on pretty normal hardware.\n",
        "\n",
        "You can find the script in the \n",
        "```\n",
        "recognition\\stylegan2-ada-python\\test_script.py\n",
        "``` \n",
        "directory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFL4CZCdeXM"
      },
      "source": [
        "# Closing Notes\n",
        "\n",
        "Hopefully you now have some cool looking brains.\n",
        "\n",
        "Thanks for reading :)"
      ]
    }
  ]
}